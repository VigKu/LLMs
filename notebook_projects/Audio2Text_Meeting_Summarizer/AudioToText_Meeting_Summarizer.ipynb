{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1) Use whisper medium model for audio --> text\n",
        "### 2) Use chat with llama3.2 1B model for text --> summary format"
      ],
      "metadata": {
        "id": "-ZNmZVZm7Xuv"
      },
      "id": "-ZNmZVZm7Xuv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio --> Text\n"
      ],
      "metadata": {
        "id": "u_3xa77Q7uZb"
      },
      "id": "u_3xa77Q7uZb"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate httpx==0.27.2"
      ],
      "metadata": {
        "id": "bGrNeAha4YLW"
      },
      "id": "bGrNeAha4YLW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, TextIteratorStreamer\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "import time"
      ],
      "metadata": {
        "id": "JutfidiV4Zbm"
      },
      "id": "JutfidiV4Zbm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L5fupaEj0zev"
      },
      "id": "L5fupaEj0zev",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47dba08d-5829-417c-9c6c-bdb35ca846a6",
      "metadata": {
        "id": "47dba08d-5829-417c-9c6c-bdb35ca846a6"
      },
      "outputs": [],
      "source": [
        "AUDIO_MODEL = \"openai/whisper-medium\"\n",
        "speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)\n",
        "speech_model.to('cuda')\n",
        "processor = AutoProcessor.from_pretrained(AUDIO_MODEL)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=speech_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch.float16,\n",
        "    device='cuda',\n",
        "    return_timestamps=True #important if audio is more than 30sec\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35d6c76-01a9-495f-ad4e-84c98e320750",
      "metadata": {
        "id": "c35d6c76-01a9-495f-ad4e-84c98e320750"
      },
      "outputs": [],
      "source": [
        "result = pipe(\"/content/sample_data/denver_extract.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fba2d46-b806-4bb3-b02d-e628343db986",
      "metadata": {
        "id": "8fba2d46-b806-4bb3-b02d-e628343db986"
      },
      "outputs": [],
      "source": [
        "transcription = result[\"text\"]\n",
        "print(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1778c4db-d003-4fb9-a0d0-6cfa71e6208d",
      "metadata": {
        "id": "1778c4db-d003-4fb9-a0d0-6cfa71e6208d"
      },
      "source": [
        "## Text --> Summary Format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "qKYUGied4Gob"
      },
      "id": "qKYUGied4Gob",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c632023-9b37-4c0d-b43a-190aacbbd80d",
      "metadata": {
        "id": "4c632023-9b37-4c0d-b43a-190aacbbd80d"
      },
      "outputs": [],
      "source": [
        "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
        "user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "Oids1aCC4-ou"
      },
      "id": "Oids1aCC4-ou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "175814b9-81b2-4f75-bf40-9ef7cac492cd",
      "metadata": {
        "id": "175814b9-81b2-4f75-bf40-9ef7cac492cd"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aaa160e-7c2b-4080-b24a-995df4469edd",
      "metadata": {
        "id": "8aaa160e-7c2b-4080-b24a-995df4469edd"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## OPTION 1\n",
        "# no thread\n",
        "streamer = TextStreamer(tokenizer)\n",
        "start = time.time()\n",
        "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "print(f\"Time taken: {time.time() - start}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_LxvFCoorQaP"
      },
      "id": "_LxvFCoorQaP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Option 2\n",
        "# include threads\n",
        "import threading\n",
        "\n",
        "# Initialize the TextIteratorStreamer for streaming output\n",
        "streamer2 = TextIteratorStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    decode_kwargs={\"skip_special_tokens\": True}\n",
        ")\n",
        "\n",
        "thread = threading.Thread(\n",
        "      target=model.generate,  # Specifies that the model's `generate` method will be run in the thread.\n",
        "      kwargs={                           # Passes the arguments required for text generation\n",
        "          \"inputs\": inputs,              # The tokenized input prompt for the model.\n",
        "          \"max_new_tokens\": 2000,  # Limits the number of tokens to be generated.\n",
        "          \"streamer\": streamer2          # The TextIteratorStreamer to handle streaming the output.\n",
        "          }\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "# Start the thread to begin the generation process\n",
        "thread.start()\n",
        "\n",
        "\n",
        "# Join main thread\n",
        "thread.join()\n",
        "\n",
        "print(f\"Time taken: {time.time() - start}\")\n",
        "\n",
        "# Stream and print the output progressively\n",
        "for text_chunk in streamer2:\n",
        "  filtered_chunk = text_chunk.replace(\"<|eot_id|>\", \"\")  # Remove special tokens if present\n",
        "  print(filtered_chunk, end=\"\")  # Print without adding new lines\n"
      ],
      "metadata": {
        "id": "PzMvv80drHpC"
      },
      "id": "PzMvv80drHpC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9tLfdo0IIaQo"
      },
      "id": "9tLfdo0IIaQo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517443aa-d230-4248-88aa-b06efd8ee3cd",
      "metadata": {
        "id": "517443aa-d230-4248-88aa-b06efd8ee3cd"
      },
      "outputs": [],
      "source": [
        "response = tokenizer.decode(outputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "0fzeU6Q_3tkM"
      },
      "id": "0fzeU6Q_3tkM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "47562f76-fd35-4eb0-a399-8e8f1fa054c3",
      "metadata": {
        "id": "47562f76-fd35-4eb0-a399-8e8f1fa054c3"
      },
      "source": [
        "## **For Markdown display**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f77fea1-0920-46e5-9230-d0e8b9f69353",
      "metadata": {
        "id": "1f77fea1-0920-46e5-9230-d0e8b9f69353"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display, update_display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ac81e2-f960-4705-aaca-2385d8aa12d6",
      "metadata": {
        "id": "35ac81e2-f960-4705-aaca-2385d8aa12d6"
      },
      "outputs": [],
      "source": [
        "display(Markdown(response))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}